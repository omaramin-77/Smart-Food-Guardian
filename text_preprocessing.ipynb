{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d270e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3185, 47)\n",
      "\n",
      "Column names:\n",
      "['url', 'product_name', 'barcode', 'brand', 'quantity', 'serving_size', 'nutriscore_letter', 'nova_group', 'ingredients_text', 'allergens', 'traces', 'energy_kcal_100g', 'fat_100g', 'saturated_fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'salt_100g', 'main_image_url', 'categories', 'contains_palm_oil', 'vegetarian_status', 'vegan_status', 'nutrient_level_fat', 'nutrient_level_saturated_fat', 'nutrient_level_sugars', 'nutrient_level_salt', 'additives', 'packaging', 'stores', 'countries', 'origins', 'manufacturing_places', 'ecoscore_grade', 'ecoscore_score', 'carbon_footprint_100g', 'additives_count', 'sugar_ratio', 'energy_density', 'protein_ratio', 'macro_balance', 'healthy_score', 'log_energy_kcal_100g', 'log_fat_100g', 'log_sugars_100g', 'log_salt_100g']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "df = pd.read_csv('preprocessedPhase1FoodFacts.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d646a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Column: brand\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 3185\n",
      "Non-null count: 3039\n",
      "Null count: 146\n",
      "Unique values: 1478\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'La Casetta di Campagna'\n",
      "  2. 'H-E-B Organics'\n",
      "  3. 'DmBio'\n",
      "  4. 'Diamond of california'\n",
      "  5. 'Tree Of Life  Inc.'\n",
      "\n",
      "============================================================\n",
      "Column: allergens\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 3185\n",
      "Non-null count: 2321\n",
      "Null count: 864\n",
      "Unique values: 243\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'Nuts'\n",
      "  2. 'Nuts'\n",
      "  3. 'Nuts, Peanuts, Soybeans'\n",
      "  4. 'Nuts'\n",
      "  5. 'Nuts'\n",
      "\n",
      "============================================================\n",
      "Column: ingredients_text\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 3185\n",
      "Non-null count: 3048\n",
      "Null count: 137\n",
      "Unique values: 2820\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'Italian: Mais'\n",
      "  2. 'German: 99,5% Linsenmehl*, 0,5 % Meersalz. aus biologischer Landwirtschaft Kann Spuren von Soja und\n",
      "  3. 'Almonds'\n",
      "  4. 'Organic whole raw almonds'\n",
      "  5. 'Almonds . soybean and/or peanut oil. sea salt.'\n",
      "\n",
      "============================================================\n",
      "Column: countries\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 3185\n",
      "Non-null count: 3181\n",
      "Null count: 4\n",
      "Unique values: 490\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'Italy'\n",
      "  2. 'United States'\n",
      "  3. 'Germany'\n",
      "  4. 'United States , World'\n",
      "  5. 'United States'\n",
      "\n",
      "============================================================\n",
      "Column: additives\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 3185\n",
      "Non-null count: 3185\n",
      "Null count: 0\n",
      "Unique values: 887\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. '[]'\n",
      "  2. '[]'\n",
      "  3. '[]'\n",
      "  4. '[]'\n",
      "  5. '[]'\n"
     ]
    }
   ],
   "source": [
    "target_columns = ['brand', 'allergens', 'ingredients_text', 'countries', 'additives']\n",
    "\n",
    "for col in target_columns:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Data type: {df[col].dtype}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Non-null count: {df[col].notna().sum()}\")\n",
    "    print(f\"Null count: {df[col].isna().sum()}\")\n",
    "    print(f\"Unique values: {df[col].nunique()}\")\n",
    "    print(f\"\\nSample values (first 5 non-null):\")\n",
    "    for idx, val in enumerate(df[col].dropna().head(5).values):\n",
    "        print(f\"  {idx+1}. {repr(val)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebe1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_NORMALIZATION = {\n",
    "    \"usa\": \"united states\",\n",
    "    \"u.s.a.\": \"united states\",\n",
    "    \"us\": \"united states\",\n",
    "    \"united states of america\": \"united states\",\n",
    "    \"uk\": \"united kingdom\",\n",
    "    \"u.k.\": \"united kingdom\",\n",
    "    \"england\": \"united kingdom\",\n",
    "    \"scotland\": \"united kingdom\",\n",
    "    \"wales\": \"united kingdom\",\n",
    "    \"gb\": \"united kingdom\",\n",
    "    \"germany\": \"germany\",\n",
    "    \"deutschland\": \"germany\",\n",
    "    \"austria\": \"austria\",\n",
    "    \"Ã¶sterreich\": \"austria\",\n",
    "    \"france\": \"france\",\n",
    "    \"espagne\": \"spain\",\n",
    "    \"spain\": \"spain\",\n",
    "    \"italy\": \"italy\",\n",
    "}\n",
    "\n",
    "COMPANY_SUFFIXES = {\n",
    "    \"inc\", \"inc.\", \"sa\", \"s.a.\", \"gmbh\", \"srl\", \"s.r.l.\",\n",
    "    \"ltd\", \"ltd.\", \"co.\", \"company\", \"ag\", \"kg\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f6da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _basic_clean(text: str) -> str:\n",
    "    \"\"\"Common text cleaning applied to all columns.\"\"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'&[a-z]+;', '', text)\n",
    "\n",
    "    text = re.sub(\n",
    "        r'^(german|french|italian|spanish|english|portuguese|dutch|swedish|danish|'\n",
    "        r'norwegian|polish|czech|hungarian|romanian|bulgarian|greek|russian|ukrainian|'\n",
    "        r'turkish|arabic|hebrew|japanese|chinese|korean):\\s*',\n",
    "        '',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    text = re.sub(r'\\[|\\]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\s*[,;]\\s*', ', ', text)\n",
    "    text = re.sub(r'\\s*\\.\\s*', '. ', text)\n",
    "\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\*{2,}', '*', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_ingredients(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ingredients: keep main content, drop parentheses with 'may contain' / 'traces' / 'contains'\n",
    "    because they are often cross-contamination metadata.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\([^)]*(may contain|traces|contains)[^)]*\\)', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b65e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_brand(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Brand: split on commas, remove duplicates and company suffixes.\n",
    "    Return a pipe-separated list of normalized brand names.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    parts = [p.strip() for p in re.split(r'[,;/]+', text) if p.strip()]\n",
    "    cleaned_brands = []\n",
    "\n",
    "    for p in parts:\n",
    "        tokens = [t for t in p.split() if t not in COMPANY_SUFFIXES]\n",
    "        if not tokens:\n",
    "            continue\n",
    "        brand_name = ' '.join(tokens)\n",
    "        cleaned_brands.append(brand_name)\n",
    "\n",
    "    seen = set()\n",
    "    unique_brands = []\n",
    "    for b in cleaned_brands:\n",
    "        if b not in seen:\n",
    "            seen.add(b)\n",
    "            unique_brands.append(b)\n",
    "\n",
    "    return ' | '.join(unique_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_allergens(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Allergens: split on commas/semicolons, normalize phrases into canonical tags.\n",
    "    Output is a pipe-separated list of allergen tags for easy ML use.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    raw_tokens = re.split(r'[;,/]+', text)\n",
    "    normalized = []\n",
    "\n",
    "    for tok in raw_tokens:\n",
    "        tok = tok.strip().lower()\n",
    "        if not tok:\n",
    "            continue\n",
    "\n",
    "        tok = re.sub(r'\\bmay contain\\b', '', tok)\n",
    "        tok = re.sub(r'\\btraces of\\b', '', tok)\n",
    "        tok = re.sub(r'\\bcontains\\b', '', tok)\n",
    "        tok = tok.strip()\n",
    "\n",
    "        if not tok:\n",
    "            continue\n",
    "\n",
    "        if 'sulphur dioxide' in tok or 'sulfites' in tok or 'sulphites' in tok:\n",
    "            label = 'sulphur_dioxide_sulphites'\n",
    "        elif 'peanut' in tok:\n",
    "            label = 'peanuts'\n",
    "        elif 'nut' in tok and 'peanut' not in tok:\n",
    "            label = 'tree_nuts'\n",
    "        elif 'milk' in tok or 'lactose' in tok or 'dairy' in tok:\n",
    "            label = 'milk'\n",
    "        elif 'egg' in tok:\n",
    "            label = 'eggs'\n",
    "        elif 'soy' in tok or 'soya' in tok:\n",
    "            label = 'soybeans'\n",
    "        elif 'gluten' in tok or 'wheat' in tok or 'barley' in tok or 'rye' in tok:\n",
    "            label = 'gluten'\n",
    "        elif 'sesame' in tok:\n",
    "            label = 'sesame'\n",
    "        elif 'fish' in tok:\n",
    "            label = 'fish'\n",
    "        elif 'crustacean' in tok or 'shrimp' in tok or 'prawn' in tok or 'crab' in tok:\n",
    "            label = 'crustaceans'\n",
    "        elif 'mustard' in tok:\n",
    "            label = 'mustard'\n",
    "        elif 'celery' in tok:\n",
    "            label = 'celery'\n",
    "        elif 'lupin' in tok:\n",
    "            label = 'lupin'\n",
    "        else:\n",
    "            label = tok\n",
    "\n",
    "        normalized.append(label)\n",
    "\n",
    "    seen = set()\n",
    "    unique_labels = []\n",
    "    for a in normalized:\n",
    "        if a not in seen:\n",
    "            seen.add(a)\n",
    "            unique_labels.append(a)\n",
    "\n",
    "    return ' | '.join(unique_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c538cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_countries(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Countries: split, normalize variants (usa -> united states), drop 'world',\n",
    "    deduplicate. Return pipe-separated list.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    parts = [p.strip().lower() for p in re.split(r'[;,/]+', text) if p.strip()]\n",
    "    normalized = []\n",
    "\n",
    "    for p in parts:\n",
    "        p = re.sub(r'\\s+', ' ', p)\n",
    "        p = COUNTRY_NORMALIZATION.get(p, p)\n",
    "\n",
    "        if p in {'world', 'en:world'}:\n",
    "            continue\n",
    "\n",
    "        normalized.append(p)\n",
    "\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in normalized:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            unique.append(c)\n",
    "\n",
    "    return ' | '.join(unique)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
