{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fce0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a0fb3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, GlobalAveragePooling1D, Dense,\n",
    "    Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, SimpleRNN, GRU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296342e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../FoodFactsCleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9229b60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TEXT_COLS = [\n",
    "    \"brand_cleaned\",\n",
    "    \"allergens_cleaned\",\n",
    "    \"ingredients_text_cleaned\",\n",
    "    \"countries_cleaned\",\n",
    "    \"additives_cleaned\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9303e87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate into a single text field per product\n",
    "df[\"text_concat\"] = df[TEXT_COLS].fillna(\"\").agg(\" \".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112d9e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_COL = \"nutriscore_letter\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0c6cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_text = df[\"text_concat\"]\n",
    "y = df[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac7028",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_text,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b872591",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Macro F1:  {f1_macro:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8574c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Store results\n",
    "results = []\n",
    "\n",
    "def log_result(name, y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Macro F1:  {f1_macro:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"=\" * 80)\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_f1\": f1_macro\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202771e",
   "metadata": {},
   "source": [
    "# PATH A: TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad62993",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_features=30000  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9de17",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58694fcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    "    multi_class=\"multinomial\",\n",
    "    class_weight=\"balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed418fd3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_tfidf)\n",
    "log_result(\"Path A1: TF-IDF + LogisticRegression\", y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a0439",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "svm_clf = LinearSVC(\n",
    "    C=1.0,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "svm_clf.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test_tfidf)\n",
    "log_result(\"Path A2: TF-IDF + LinearSVC\", y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13ee34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- TF-IDF + (Neural Network) ----\n",
    "\n",
    "# Convert sparse to dense for Keras (be careful with very high dim)\n",
    "X_train_dense = X_train_tfidf.toarray()\n",
    "X_test_dense = X_test_tfidf.toarray()\n",
    "\n",
    "# Encode labels as integers for Keras\n",
    "le_tfidf = LabelEncoder()\n",
    "y_train_enc = le_tfidf.fit_transform(y_train)\n",
    "y_test_enc = le_tfidf.transform(y_test)\n",
    "num_classes = len(le_tfidf.classes_)\n",
    "\n",
    "input_dim = X_train_dense.shape[1]\n",
    "\n",
    "def build_tfidf_mlp(input_dim, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation=\"relu\", input_shape=(input_dim,)),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3ffff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mlp_tfidf = build_tfidf_mlp(input_dim, num_classes)\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "history_mlp = mlp_tfidf.fit(\n",
    "    X_train_dense,\n",
    "    y_train_enc,\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4885181",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_proba_mlp = mlp_tfidf.predict(X_test_dense)\n",
    "y_pred_mlp_enc = np.argmax(y_proba_mlp, axis=1)\n",
    "y_pred_mlp = le_tfidf.inverse_transform(y_pred_mlp_enc)\n",
    "\n",
    "log_result(\"Path A3: TF-IDF + Custom MLP\", y_test, y_pred_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811754f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values for TF-IDF MLP\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_mlp.history['accuracy'])\n",
    "plt.plot(history_mlp.history['val_accuracy'])\n",
    "plt.title('TF-IDF MLP Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_mlp.history['loss'])\n",
    "plt.plot(history_mlp.history['val_loss'])\n",
    "plt.title('TF-IDF MLP Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6367f",
   "metadata": {},
   "source": [
    "# PATH B: Neural Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87acaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Encode labels to integers (for neural net) ----\n",
    "le_seq = LabelEncoder()\n",
    "y_train_seq_enc = le_seq.fit_transform(y_train)\n",
    "y_test_seq_enc = le_seq.transform(y_test)\n",
    "num_classes_seq = len(le_seq.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac34fe4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Number of classes:\", num_classes)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901c09d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- 2) Tokenize & pad sequences ----\n",
    "MAX_WORDS = 30000   # vocab size\n",
    "MAX_LEN = 200       # max tokens per sample\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf110772",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747de9a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- 3) Build the model ----\n",
    "def build_baseline_text_model(vocab_size, max_len, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea73e3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_rnn_text_model(vocab_size, max_len, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
    "        SimpleRNN(64, return_sequences=False),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd665b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_cnn_text_model(vocab_size, max_len, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
    "        Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.4),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a90b87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "es_seq = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d8fbb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- Baseline Embedding + GlobalAveragePooling ----\n",
    "model_b1 = build_baseline_text_model(vocab_size, MAX_LEN, num_classes_seq)\n",
    "\n",
    "history_b1 = model_b1.fit(\n",
    "    X_train_pad,\n",
    "    y_train_seq_enc,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_proba_b1 = model_b1.predict(X_test_pad)\n",
    "y_pred_b1_enc = np.argmax(y_proba_b1, axis=1)\n",
    "y_pred_b1 = le_seq.inverse_transform(y_pred_b1_enc)\n",
    "\n",
    "log_result(\"Embedding + GlobalAveragePooling\", y_test, y_pred_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08792173",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values for Baseline model\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_b1.history['accuracy'])\n",
    "plt.plot(history_b1.history['val_accuracy'])\n",
    "plt.title('Embedding + GlobalAveragePooling accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_b1.history['loss'])\n",
    "plt.plot(history_b1.history['val_loss'])\n",
    "plt.title('Embedding + GlobalAveragePooling loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256142e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- CNN-based text model (TextCNN-style) ----\n",
    "\n",
    "model_b3 = build_cnn_text_model(vocab_size, MAX_LEN, num_classes_seq)\n",
    "\n",
    "history_b3 = model_b3.fit(\n",
    "    X_train_pad,\n",
    "    y_train_seq_enc,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    callbacks=[es_seq],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_proba_b3 = model_b3.predict(X_test_pad)\n",
    "y_pred_b3_enc = np.argmax(y_proba_b3, axis=1)\n",
    "y_pred_b3 = le_seq.inverse_transform(y_pred_b3_enc)\n",
    "\n",
    "log_result(\"Path B3: Embedding + Conv1D\", y_test, y_pred_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ac1e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values for CNN model\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_b3.history['accuracy'])\n",
    "plt.plot(history_b3.history['val_accuracy'])\n",
    "plt.title('Embedding + Conv1D accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_b3.history['loss'])\n",
    "plt.plot(history_b3.history['val_loss'])\n",
    "plt.title('Embedding + Conv1D loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
