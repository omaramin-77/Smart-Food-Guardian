{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d270e7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5651, 47)\n",
      "\n",
      "Column names:\n",
      "['url', 'product_name', 'barcode', 'brand', 'quantity', 'serving_size', 'nutriscore_letter', 'nova_group', 'ingredients_text', 'allergens', 'traces', 'energy_kcal_100g', 'fat_100g', 'saturated_fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', 'salt_100g', 'main_image_url', 'categories', 'contains_palm_oil', 'vegetarian_status', 'vegan_status', 'nutrient_level_fat', 'nutrient_level_saturated_fat', 'nutrient_level_sugars', 'nutrient_level_salt', 'additives', 'packaging', 'stores', 'countries', 'origins', 'manufacturing_places', 'ecoscore_grade', 'ecoscore_score', 'carbon_footprint_100g', 'additives_count', 'sugar_ratio', 'energy_density', 'protein_ratio', 'macro_balance', 'healthy_score', 'log_energy_kcal_100g', 'log_fat_100g', 'log_sugars_100g', 'log_salt_100g']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "df = pd.read_csv('preprocessedPhase1FoodFacts.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d646a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Column: brand\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 5651\n",
      "Non-null count: 5410\n",
      "Null count: 241\n",
      "Unique values: 2445\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'La Casetta di Campagna'\n",
      "  2. 'H-E-B Organics'\n",
      "  3. 'DmBio'\n",
      "  4. 'Diamond of california'\n",
      "  5. 'Tree Of Life  Inc.'\n",
      "\n",
      "============================================================\n",
      "Column: allergens\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 5651\n",
      "Non-null count: 3672\n",
      "Null count: 1979\n",
      "Unique values: 364\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'Nuts'\n",
      "  2. 'Nuts'\n",
      "  3. 'Nuts, Peanuts, Soybeans'\n",
      "  4. 'Nuts'\n",
      "  5. 'Nuts'\n",
      "\n",
      "============================================================\n",
      "Column: ingredients_text\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 5651\n",
      "Non-null count: 5267\n",
      "Null count: 384\n",
      "Unique values: 4911\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'Italian: Mais'\n",
      "  2. 'German: 99,5% Linsenmehl*, 0,5 % Meersalz. aus biologischer Landwirtschaft Kann Spuren von Soja und\n",
      "  3. 'Almonds'\n",
      "  4. 'Organic whole raw almonds'\n",
      "  5. 'Almonds . soybean and/or peanut oil. sea salt.'\n",
      "\n",
      "============================================================\n",
      "Column: countries\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 5651\n",
      "Non-null count: 5639\n",
      "Null count: 12\n",
      "Unique values: 615\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. 'Italy'\n",
      "  2. 'United States'\n",
      "  3. 'Germany'\n",
      "  4. 'United States , World'\n",
      "  5. 'United States'\n",
      "\n",
      "============================================================\n",
      "Column: additives\n",
      "============================================================\n",
      "Data type: object\n",
      "Total rows: 5651\n",
      "Non-null count: 5651\n",
      "Null count: 0\n",
      "Unique values: 1518\n",
      "\n",
      "Sample values (first 5 non-null):\n",
      "  1. '[]'\n",
      "  2. '[]'\n",
      "  3. '[]'\n",
      "  4. '[]'\n",
      "  5. '[]'\n"
     ]
    }
   ],
   "source": [
    "target_columns = ['brand', 'allergens', 'ingredients_text', 'countries', 'additives']\n",
    "\n",
    "for col in target_columns:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Data type: {df[col].dtype}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Non-null count: {df[col].notna().sum()}\")\n",
    "    print(f\"Null count: {df[col].isna().sum()}\")\n",
    "    print(f\"Unique values: {df[col].nunique()}\")\n",
    "    print(f\"\\nSample values (first 5 non-null):\")\n",
    "    for idx, val in enumerate(df[col].dropna().head(5).values):\n",
    "        print(f\"  {idx+1}. {repr(val)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebe1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_NORMALIZATION = {\n",
    "    \"usa\": \"united states\",\n",
    "    \"u.s.a.\": \"united states\",\n",
    "    \"us\": \"united states\",\n",
    "    \"united states of america\": \"united states\",\n",
    "    \"uk\": \"united kingdom\",\n",
    "    \"u.k.\": \"united kingdom\",\n",
    "    \"england\": \"united kingdom\",\n",
    "    \"scotland\": \"united kingdom\",\n",
    "    \"wales\": \"united kingdom\",\n",
    "    \"gb\": \"united kingdom\",\n",
    "    \"germany\": \"germany\",\n",
    "    \"deutschland\": \"germany\",\n",
    "    \"austria\": \"austria\",\n",
    "    \"österreich\": \"austria\",\n",
    "    \"france\": \"france\",\n",
    "    \"espagne\": \"spain\",\n",
    "    \"spain\": \"spain\",\n",
    "    \"italy\": \"italy\",\n",
    "}\n",
    "\n",
    "COMPANY_SUFFIXES = {\n",
    "    \"inc\", \"inc.\", \"sa\", \"s.a.\", \"gmbh\", \"srl\", \"s.r.l.\",\n",
    "    \"ltd\", \"ltd.\", \"co.\", \"company\", \"ag\", \"kg\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f6da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _basic_clean(text: str) -> str:\n",
    "    \"\"\"Common text cleaning applied to all columns.\"\"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'&[a-z]+;', '', text)\n",
    "\n",
    "    text = re.sub(\n",
    "        r'^(german|french|italian|spanish|english|portuguese|dutch|swedish|danish|'\n",
    "        r'norwegian|polish|czech|hungarian|romanian|bulgarian|greek|russian|ukrainian|'\n",
    "        r'turkish|arabic|hebrew|japanese|chinese|korean):\\s*',\n",
    "        '',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    text = re.sub(r'\\[|\\]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\s*[,;]\\s*', ', ', text)\n",
    "    text = re.sub(r'\\s*\\.\\s*', '. ', text)\n",
    "\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\*{2,}', '*', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6094fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_ingredients(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Ingredients: keep main content, drop parentheses with 'may contain' / 'traces' / 'contains'\n",
    "    because they are often cross-contamination metadata.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\([^)]*(may contain|traces|contains)[^)]*\\)', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b65e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_brand(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Brand: split on commas, remove duplicates and company suffixes.\n",
    "    Return a pipe-separated list of normalized brand names.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    parts = [p.strip() for p in re.split(r'[,;/]+', text) if p.strip()]\n",
    "    cleaned_brands = []\n",
    "\n",
    "    for p in parts:\n",
    "        tokens = [t for t in p.split() if t not in COMPANY_SUFFIXES]\n",
    "        if not tokens:\n",
    "            continue\n",
    "        brand_name = ' '.join(tokens)\n",
    "        cleaned_brands.append(brand_name)\n",
    "\n",
    "    seen = set()\n",
    "    unique_brands = []\n",
    "    for b in cleaned_brands:\n",
    "        if b not in seen:\n",
    "            seen.add(b)\n",
    "            unique_brands.append(b)\n",
    "\n",
    "    return ' | '.join(unique_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_allergens(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Allergens: split on commas/semicolons, normalize phrases into canonical tags.\n",
    "    Output is a pipe-separated list of allergen tags for easy ML use.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    raw_tokens = re.split(r'[;,/]+', text)\n",
    "    normalized = []\n",
    "\n",
    "    for tok in raw_tokens:\n",
    "        tok = tok.strip().lower()\n",
    "        if not tok:\n",
    "            continue\n",
    "\n",
    "        tok = re.sub(r'\\bmay contain\\b', '', tok)\n",
    "        tok = re.sub(r'\\btraces of\\b', '', tok)\n",
    "        tok = re.sub(r'\\bcontains\\b', '', tok)\n",
    "        tok = tok.strip()\n",
    "\n",
    "        if not tok:\n",
    "            continue\n",
    "\n",
    "        if 'sulphur dioxide' in tok or 'sulfites' in tok or 'sulphites' in tok:\n",
    "            label = 'sulphur_dioxide_sulphites'\n",
    "        elif 'peanut' in tok:\n",
    "            label = 'peanuts'\n",
    "        elif 'nut' in tok and 'peanut' not in tok:\n",
    "            label = 'tree_nuts'\n",
    "        elif 'milk' in tok or 'lactose' in tok or 'dairy' in tok:\n",
    "            label = 'milk'\n",
    "        elif 'egg' in tok:\n",
    "            label = 'eggs'\n",
    "        elif 'soy' in tok or 'soya' in tok:\n",
    "            label = 'soybeans'\n",
    "        elif 'gluten' in tok or 'wheat' in tok or 'barley' in tok or 'rye' in tok:\n",
    "            label = 'gluten'\n",
    "        elif 'sesame' in tok:\n",
    "            label = 'sesame'\n",
    "        elif 'fish' in tok:\n",
    "            label = 'fish'\n",
    "        elif 'crustacean' in tok or 'shrimp' in tok or 'prawn' in tok or 'crab' in tok:\n",
    "            label = 'crustaceans'\n",
    "        elif 'mustard' in tok:\n",
    "            label = 'mustard'\n",
    "        elif 'celery' in tok:\n",
    "            label = 'celery'\n",
    "        elif 'lupin' in tok:\n",
    "            label = 'lupin'\n",
    "        else:\n",
    "            label = tok\n",
    "\n",
    "        normalized.append(label)\n",
    "\n",
    "    seen = set()\n",
    "    unique_labels = []\n",
    "    for a in normalized:\n",
    "        if a not in seen:\n",
    "            seen.add(a)\n",
    "            unique_labels.append(a)\n",
    "\n",
    "    return ' | '.join(unique_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c538cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_countries(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Countries: split, normalize variants (usa -> united states), drop 'world',\n",
    "    deduplicate. Return pipe-separated list.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    parts = [p.strip().lower() for p in re.split(r'[;,/]+', text) if p.strip()]\n",
    "    normalized = []\n",
    "\n",
    "    for p in parts:\n",
    "        p = re.sub(r'\\s+', ' ', p)\n",
    "        p = COUNTRY_NORMALIZATION.get(p, p)\n",
    "\n",
    "        if p in {'world', 'en:world'}:\n",
    "            continue\n",
    "\n",
    "        normalized.append(p)\n",
    "\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for c in normalized:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            unique.append(c)\n",
    "\n",
    "    return ' | '.join(unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ce7f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_additives(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Additives: parse list-like strings and extract E-codes.\n",
    "    Output: space-separated list of unique additive codes (e.g., 'e150d e621').\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    s = text.strip()\n",
    "    items = []\n",
    "\n",
    "    if s.startswith('[') and s.endswith(']'):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(s)\n",
    "            if isinstance(parsed, str):\n",
    "                items = [parsed]\n",
    "            elif isinstance(parsed, (list, tuple)):\n",
    "                items = [str(x) for x in parsed]\n",
    "            else:\n",
    "                items = [s]\n",
    "        except Exception:\n",
    "            items = re.split(r'[;,]+', s)\n",
    "    else:\n",
    "        items = re.split(r'[;,]+', s)\n",
    "\n",
    "    codes = []\n",
    "\n",
    "    for item in items:\n",
    "        item = str(item).lower()\n",
    "        item = item.strip()\n",
    "        if not item:\n",
    "            continue\n",
    "        m = re.search(r'(e\\s*\\d+[a-z]?)', item, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            code = m.group(1).lower().replace(' ', '')\n",
    "            codes.append(code)\n",
    "\n",
    "    codes = sorted(set(codes))\n",
    "\n",
    "    return ' '.join(codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8836fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: Optional[str], column_type: str = 'general') -> str:\n",
    "    \"\"\"\n",
    "    Column-aware text preprocessing tailored to this dataset.\n",
    "\n",
    "    Args:\n",
    "        text: raw cell value\n",
    "        column_type: one of 'brand', 'allergens', 'ingredients_text',\n",
    "                     'countries', 'additives', or 'general'.\n",
    "\n",
    "    Returns:\n",
    "        Cleaned / normalized text suitable for downstream ML.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return ''\n",
    "\n",
    "    text = str(text).strip()\n",
    "    if not text:\n",
    "        return ''\n",
    "\n",
    "    base = _basic_clean(text)\n",
    "\n",
    "    if column_type == 'brand':\n",
    "        return _normalize_brand(base)\n",
    "\n",
    "    if column_type == 'allergens':\n",
    "        return _normalize_allergens(base)\n",
    "\n",
    "    if column_type == 'ingredients_text':\n",
    "        return _clean_ingredients(base)\n",
    "\n",
    "    if column_type == 'countries':\n",
    "        return _normalize_countries(base)\n",
    "\n",
    "    if column_type == 'additives':\n",
    "        return _normalize_additives(base)\n",
    "\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd30f9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing brand...\n",
      "  ✓ Created brand_cleaned\n",
      "Processing allergens...\n",
      "  ✓ Created allergens_cleaned\n",
      "Processing ingredients_text...\n",
      "  ✓ Created ingredients_text_cleaned\n",
      "Processing countries...\n",
      "  ✓ Created countries_cleaned\n",
      "Processing additives...\n",
      "  ✓ Created additives_cleaned\n"
     ]
    }
   ],
   "source": [
    "target_columns = ['brand', 'allergens', 'ingredients_text', 'countries', 'additives']\n",
    "\n",
    "for col in target_columns:\n",
    "    new_col_name = f'{col}_cleaned'\n",
    "    print(f\"Processing {col}...\")\n",
    "    df[new_col_name] = df[col].apply(lambda x: preprocess_text(x, column_type=col))\n",
    "    print(f\"  ✓ Created {new_col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b11222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample comparison (first 5 rows with non-empty values):\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "Row 0:\n",
      "  brand:\n",
      "    Original: 'La Casetta di Campagna'\n",
      "    Cleaned:  'la casetta di campagna'\n",
      "  ingredients_text:\n",
      "    Original: 'Italian: Mais'\n",
      "    Cleaned:  'mais'\n",
      "  countries:\n",
      "    Original: 'Italy'\n",
      "    Cleaned:  'italy'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 1:\n",
      "  brand:\n",
      "    Original: 'H-E-B Organics'\n",
      "    Cleaned:  'h-e-b organics'\n",
      "  countries:\n",
      "    Original: 'United States'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 2:\n",
      "  brand:\n",
      "    Original: 'DmBio'\n",
      "    Cleaned:  'dmbio'\n",
      "  ingredients_text:\n",
      "    Original: 'German: 99,5% Linsenmehl*, 0,5 % Meersalz. aus biologischer Landwirtsc'\n",
      "    Cleaned:  '99, 5% linsenmehl*, 0, 5 % meersalz. aus biologischer landwirtschaft k'\n",
      "  countries:\n",
      "    Original: 'Germany'\n",
      "    Cleaned:  'germany'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 3:\n",
      "  brand:\n",
      "    Original: 'Diamond of california'\n",
      "    Cleaned:  'diamond of california'\n",
      "  allergens:\n",
      "    Original: 'Nuts'\n",
      "    Cleaned:  'tree_nuts'\n",
      "  ingredients_text:\n",
      "    Original: 'Almonds'\n",
      "    Cleaned:  'almonds'\n",
      "  countries:\n",
      "    Original: 'United States , World'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 4:\n",
      "  brand:\n",
      "    Original: 'Tree Of Life  Inc.'\n",
      "    Cleaned:  'tree of life'\n",
      "  allergens:\n",
      "    Original: 'Nuts'\n",
      "    Cleaned:  'tree_nuts'\n",
      "  ingredients_text:\n",
      "    Original: 'Organic whole raw almonds'\n",
      "    Cleaned:  'organic whole raw almonds'\n",
      "  countries:\n",
      "    Original: 'United States'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 5:\n",
      "  brand:\n",
      "    Original: 'Johnvince Foods'\n",
      "    Cleaned:  'johnvince foods'\n",
      "  allergens:\n",
      "    Original: 'Nuts, Peanuts, Soybeans'\n",
      "    Cleaned:  'tree_nuts | peanuts | soybeans'\n",
      "  ingredients_text:\n",
      "    Original: 'Almonds . soybean and/or peanut oil. sea salt.'\n",
      "    Cleaned:  'almonds. soybean and/or peanut oil. sea salt.'\n",
      "  countries:\n",
      "    Original: 'United States'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 6:\n",
      "  brand:\n",
      "    Original: 'Winco Foods , Winco Foods  Inc.'\n",
      "    Cleaned:  'winco foods'\n",
      "  allergens:\n",
      "    Original: 'Nuts'\n",
      "    Cleaned:  'tree_nuts'\n",
      "  ingredients_text:\n",
      "    Original: 'Almonds .'\n",
      "    Cleaned:  'almonds.'\n",
      "  countries:\n",
      "    Original: 'United States'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 7:\n",
      "  brand:\n",
      "    Original: 'Green Way , Kohl Corporation'\n",
      "    Cleaned:  'green way | kohl corporation'\n",
      "  allergens:\n",
      "    Original: 'Nuts'\n",
      "    Cleaned:  'tree_nuts'\n",
      "  ingredients_text:\n",
      "    Original: 'Organic almonds (not roasted).'\n",
      "    Cleaned:  'organic almonds (not roasted).'\n",
      "  countries:\n",
      "    Original: 'United States'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 8:\n",
      "  brand:\n",
      "    Original: 'Green Way , Kohl Corporation'\n",
      "    Cleaned:  'green way | kohl corporation'\n",
      "  allergens:\n",
      "    Original: 'Nuts'\n",
      "    Cleaned:  'tree_nuts'\n",
      "  ingredients_text:\n",
      "    Original: 'Organic almonds (roasted in organic high oleic sunflower oil).'\n",
      "    Cleaned:  'organic almonds (roasted in organic high oleic sunflower oil).'\n",
      "  countries:\n",
      "    Original: 'United States'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n",
      "\n",
      "Row 9:\n",
      "  brand:\n",
      "    Original: 'Fisher'\n",
      "    Cleaned:  'fisher'\n",
      "  allergens:\n",
      "    Original: 'Nuts'\n",
      "    Cleaned:  'tree_nuts'\n",
      "  ingredients_text:\n",
      "    Original: 'Almonds .'\n",
      "    Cleaned:  'almonds.'\n",
      "  countries:\n",
      "    Original: 'United States'\n",
      "    Cleaned:  'united states'\n",
      "  additives:\n",
      "    Original: '[]'\n",
      "    Cleaned:  ''\n"
     ]
    }
   ],
   "source": [
    "comparison_cols = []\n",
    "for col in target_columns:\n",
    "    comparison_cols.extend([col, f'{col}_cleaned'])\n",
    "\n",
    "print(\"Sample comparison (first 5 rows with non-empty values):\")\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "sample_df = df[comparison_cols].head(10)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    has_data = False\n",
    "    for col in target_columns:\n",
    "        if pd.notna(row[col]) and str(row[col]).strip():\n",
    "            has_data = True\n",
    "            break\n",
    "    \n",
    "    if has_data:\n",
    "        print(f\"\\nRow {idx}:\")\n",
    "        for col in target_columns:\n",
    "            orig = row[col]\n",
    "            clean = row[f'{col}_cleaned']\n",
    "            if pd.notna(orig) and str(orig).strip():\n",
    "                print(f\"  {col}:\")\n",
    "                print(f\"    Original: {repr(str(orig)[:70])}\")\n",
    "                print(f\"    Cleaned:  {repr(clean[:70])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292faaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Summary Statistics\n",
      "======================================================================\n",
      "\n",
      "brand:\n",
      "  Original non-null values: 5410\n",
      "  Original non-empty values: 5651\n",
      "  Cleaned non-empty values: 5410\n",
      "  Unique values (original): 2445\n",
      "  Unique values (cleaned): 2241\n",
      "  Average length (original): 12.2\n",
      "  Average length (cleaned): 11.7\n",
      "\n",
      "allergens:\n",
      "  Original non-null values: 3672\n",
      "  Original non-empty values: 5651\n",
      "  Cleaned non-empty values: 3672\n",
      "  Unique values (original): 364\n",
      "  Unique values (cleaned): 344\n",
      "  Average length (original): 11.5\n",
      "  Average length (cleaned): 11.6\n",
      "\n",
      "ingredients_text:\n",
      "  Original non-null values: 5267\n",
      "  Original non-empty values: 5651\n",
      "  Cleaned non-empty values: 5267\n",
      "  Unique values (original): 4911\n",
      "  Unique values (cleaned): 4876\n",
      "  Average length (original): 245.6\n",
      "  Average length (cleaned): 240.5\n",
      "\n",
      "countries:\n",
      "  Original non-null values: 5639\n",
      "  Original non-empty values: 5651\n",
      "  Cleaned non-empty values: 5636\n",
      "  Unique values (original): 615\n",
      "  Unique values (cleaned): 592\n",
      "  Average length (original): 14.3\n",
      "  Average length (cleaned): 13.8\n",
      "\n",
      "additives:\n",
      "  Original non-null values: 5651\n",
      "  Original non-empty values: 5651\n",
      "  Cleaned non-empty values: 3027\n",
      "  Unique values (original): 1518\n",
      "  Unique values (cleaned): 1497\n",
      "  Average length (original): 54.8\n",
      "  Average length (cleaned): 10.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing Summary Statistics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for col in target_columns:\n",
    "    original = df[col]\n",
    "    cleaned = df[f'{col}_cleaned']\n",
    "    \n",
    "    orig_non_null = original.notna().sum()\n",
    "    orig_non_empty = (original.astype(str).str.strip() != '').sum()\n",
    "    clean_non_empty = (cleaned.str.strip() != '').sum()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Original non-null values: {orig_non_null}\")\n",
    "    print(f\"  Original non-empty values: {orig_non_empty}\")\n",
    "    print(f\"  Cleaned non-empty values: {clean_non_empty}\")\n",
    "    print(f\"  Unique values (original): {original.nunique()}\")\n",
    "    print(f\"  Unique values (cleaned): {cleaned.nunique()}\")\n",
    "    print(f\"  Average length (original): {original.astype(str).str.len().mean():.1f}\")\n",
    "    print(f\"  Average length (cleaned): {cleaned.str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597b291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"preprocessed_foodfacts_phase2.csv\"\n",
    "\n",
    "df.to_csv(output_path, index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
