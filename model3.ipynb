{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2768030b",
   "metadata": {},
   "source": [
    "# Multimodal Late Fusion Model\n",
    "## Text + Tabular + Image embeddings -> SVM + XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5498683",
   "metadata": {},
   "source": [
    "## 0) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23246ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HFCS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Bidirectional, LSTM,\n",
    "    Dense, Dropout, BatchNormalization,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16b472",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9ea3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"FoodFactsCleaned.csv\")\n",
    "df[\"nutriscore_letter\"] = df[\"nutriscore_letter\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abd7f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLS = [ \n",
    "    \"brand_cleaned\",\n",
    "    \"allergens_cleaned\",\n",
    "    \"ingredients_text_cleaned\",\n",
    "    \"countries_cleaned\",\n",
    "    \"additives_cleaned\",\n",
    "]\n",
    "\n",
    "TABULAR_COLS = [\n",
    "    'nova_group', 'fat_100g',\n",
    "    'saturated_fat_100g', 'carbohydrates_100g', 'sugars_100g', 'fiber_100g',\n",
    "    'proteins_100g', 'contains_palm_oil', 'vegetarian_status', 'vegan_status',\n",
    "    'nutrient_level_fat', 'nutrient_level_saturated_fat',\n",
    "    'nutrient_level_sugars', 'nutrient_level_salt', 'ecoscore_grade', 'ecoscore_score',\n",
    "    'carbon_footprint_100g', 'additives_count', 'sugar_ratio',\n",
    "    'energy_density', 'protein_ratio', 'macro_balance', 'healthy_score',\n",
    "    'log_energy_kcal_100g', 'log_salt_100g'\n",
    "]\n",
    "\n",
    "TARGET_COL = \"nutriscore_letter\"           \n",
    "IMAGE_COL = \"image_160_path\"\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Text tokenization\n",
    "MAX_WORDS = 30000\n",
    "MAX_LEN = 200\n",
    "\n",
    "# Image settings\n",
    "IMG_SIZE = (160, 160)\n",
    "\n",
    "# NN training\n",
    "EPOCHS_TEXT = 10\n",
    "EPOCHS_TAB  = 25\n",
    "EPOCHS_IMG  = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4f1cf",
   "metadata": {},
   "source": [
    "## 2) Basic checks + building concatenated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe39424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after filtering invalid image paths: 5138\n"
     ]
    }
   ],
   "source": [
    "needed = TEXT_COLS + TABULAR_COLS + [TARGET_COL, IMAGE_COL]\n",
    "missing = [c for c in needed if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in df: {missing}\")\n",
    "\n",
    "for c in TEXT_COLS:\n",
    "    df[c] = df[c].fillna(\"\").astype(str)\n",
    "\n",
    "df[\"text_concat\"] = df[TEXT_COLS].agg(\" \".join, axis=1)\n",
    "\n",
    "df = df[df[IMAGE_COL].notna()].copy()\n",
    "df = df[df[IMAGE_COL].astype(str).str.len() > 0].copy()\n",
    "df = df[df[IMAGE_COL].apply(lambda p: os.path.exists(str(p)))].copy()\n",
    "\n",
    "print(\"Rows after filtering invalid image paths:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef849e",
   "metadata": {},
   "source": [
    "## 3) Unified Data Splitting across Modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c510b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 5\n",
      "Train: 4110 Test: 1028\n"
     ]
    }
   ],
   "source": [
    "X_text = df[\"text_concat\"].values\n",
    "X_tab  = df[TABULAR_COLS].values.astype(np.float32)\n",
    "X_img  = df[IMAGE_COL].astype(str).values\n",
    "y_raw  = df[TARGET_COL].values\n",
    "\n",
    "if y_raw.dtype == object or isinstance(y_raw[0], str):\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_raw)\n",
    "else:\n",
    "    y = y_raw.astype(int)\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Classes:\", num_classes)\n",
    "\n",
    "X_text_tr, X_text_te, X_tab_tr, X_tab_te, X_img_tr, X_img_te, y_tr, y_te = train_test_split(\n",
    "    X_text, X_tab, X_img, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(y_tr), \"Test:\", len(y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05972b9",
   "metadata": {},
   "source": [
    "## Text & Tabular Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77f7652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14981\n",
      "Tab dim: 25\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_text_tr)\n",
    "\n",
    "seq_tr = tokenizer.texts_to_sequences(X_text_tr)\n",
    "seq_te = tokenizer.texts_to_sequences(X_text_te)\n",
    "\n",
    "X_text_tr_pad = pad_sequences(seq_tr, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_text_te_pad = pad_sequences(seq_te, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tab_tr_sc = scaler.fit_transform(X_tab_tr).astype(np.float32)\n",
    "X_tab_te_sc = scaler.transform(X_tab_te).astype(np.float32)\n",
    "\n",
    "tab_dim = X_tab_tr_sc.shape[1]\n",
    "print(\"Tab dim:\", tab_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
